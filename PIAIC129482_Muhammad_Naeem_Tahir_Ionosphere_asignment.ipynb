{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PIAIC129482-Muhammad-Naeem-Tahir-Ionosphere-asignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNPj3pQtFYHlybRO9BVyZf+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muhammad-naeem-tahir/Assignments/blob/main/PIAIC129482_Muhammad_Naeem_Tahir_Ionosphere_asignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2vRgUyos7_x"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCkgewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwogICAgICBwZXJjZW50LnRleHRDb250ZW50ID0KICAgICAgICAgIGAke01hdGgucm91bmQoKHBvc2l0aW9uIC8gZmlsZURhdGEuYnl0ZUxlbmd0aCkgKiAxMDApfSUgZG9uZWA7CiAgICB9CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "id": "gjEtiBsbvzCK",
        "outputId": "77c3cd0f-234d-4f79-99f0-dfec1bb89a4f"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-016f798e-6865-487e-bef3-f9cfc9b4aec4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-016f798e-6865-487e-bef3-f9cfc9b4aec4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving ionosphere_data.csv to ionosphere_data.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZwnNFdI2wjl"
      },
      "source": [
        "import io\n",
        "df = pd.read_csv(io.BytesIO(uploaded['ionosphere_data.csv']))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7gt0loz3SS_",
        "outputId": "2a499609-c367-4de4-a086-78e733d78358"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(351, 35)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "QfMqbrVA3jaY",
        "outputId": "6679efb6-a97f-483d-e81e-7c5043cba602"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature1</th>\n",
              "      <th>feature2</th>\n",
              "      <th>feature3</th>\n",
              "      <th>feature4</th>\n",
              "      <th>feature5</th>\n",
              "      <th>feature6</th>\n",
              "      <th>feature7</th>\n",
              "      <th>feature8</th>\n",
              "      <th>feature9</th>\n",
              "      <th>feature10</th>\n",
              "      <th>feature11</th>\n",
              "      <th>feature12</th>\n",
              "      <th>feature13</th>\n",
              "      <th>feature14</th>\n",
              "      <th>feature15</th>\n",
              "      <th>feature16</th>\n",
              "      <th>feature17</th>\n",
              "      <th>feature18</th>\n",
              "      <th>feature19</th>\n",
              "      <th>feature20</th>\n",
              "      <th>feature21</th>\n",
              "      <th>feature22</th>\n",
              "      <th>feature23</th>\n",
              "      <th>feature24</th>\n",
              "      <th>feature25</th>\n",
              "      <th>feature26</th>\n",
              "      <th>feature27</th>\n",
              "      <th>feature28</th>\n",
              "      <th>feature29</th>\n",
              "      <th>feature30</th>\n",
              "      <th>feature31</th>\n",
              "      <th>feature32</th>\n",
              "      <th>feature33</th>\n",
              "      <th>feature34</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.99539</td>\n",
              "      <td>-0.05889</td>\n",
              "      <td>0.85243</td>\n",
              "      <td>0.02306</td>\n",
              "      <td>0.83398</td>\n",
              "      <td>-0.37708</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.03760</td>\n",
              "      <td>0.85243</td>\n",
              "      <td>-0.17755</td>\n",
              "      <td>0.59755</td>\n",
              "      <td>-0.44945</td>\n",
              "      <td>0.60536</td>\n",
              "      <td>-0.38223</td>\n",
              "      <td>0.84356</td>\n",
              "      <td>-0.38542</td>\n",
              "      <td>0.58212</td>\n",
              "      <td>-0.32192</td>\n",
              "      <td>0.56971</td>\n",
              "      <td>-0.29674</td>\n",
              "      <td>0.36946</td>\n",
              "      <td>-0.47357</td>\n",
              "      <td>0.56811</td>\n",
              "      <td>-0.51171</td>\n",
              "      <td>0.41078</td>\n",
              "      <td>-0.46168</td>\n",
              "      <td>0.21266</td>\n",
              "      <td>-0.34090</td>\n",
              "      <td>0.42267</td>\n",
              "      <td>-0.54487</td>\n",
              "      <td>0.18641</td>\n",
              "      <td>-0.45300</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.18829</td>\n",
              "      <td>0.93035</td>\n",
              "      <td>-0.36156</td>\n",
              "      <td>-0.10868</td>\n",
              "      <td>-0.93597</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.04549</td>\n",
              "      <td>0.50874</td>\n",
              "      <td>-0.67743</td>\n",
              "      <td>0.34432</td>\n",
              "      <td>-0.69707</td>\n",
              "      <td>-0.51685</td>\n",
              "      <td>-0.97515</td>\n",
              "      <td>0.05499</td>\n",
              "      <td>-0.62237</td>\n",
              "      <td>0.33109</td>\n",
              "      <td>-1.00000</td>\n",
              "      <td>-0.13151</td>\n",
              "      <td>-0.45300</td>\n",
              "      <td>-0.18056</td>\n",
              "      <td>-0.35734</td>\n",
              "      <td>-0.20332</td>\n",
              "      <td>-0.26569</td>\n",
              "      <td>-0.20468</td>\n",
              "      <td>-0.18401</td>\n",
              "      <td>-0.19040</td>\n",
              "      <td>-0.11593</td>\n",
              "      <td>-0.16626</td>\n",
              "      <td>-0.06288</td>\n",
              "      <td>-0.13738</td>\n",
              "      <td>-0.02447</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.03365</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.00485</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.12062</td>\n",
              "      <td>0.88965</td>\n",
              "      <td>0.01198</td>\n",
              "      <td>0.73082</td>\n",
              "      <td>0.05346</td>\n",
              "      <td>0.85443</td>\n",
              "      <td>0.00827</td>\n",
              "      <td>0.54591</td>\n",
              "      <td>0.00299</td>\n",
              "      <td>0.83775</td>\n",
              "      <td>-0.13644</td>\n",
              "      <td>0.75535</td>\n",
              "      <td>-0.08540</td>\n",
              "      <td>0.70887</td>\n",
              "      <td>-0.27502</td>\n",
              "      <td>0.43385</td>\n",
              "      <td>-0.12062</td>\n",
              "      <td>0.57528</td>\n",
              "      <td>-0.40220</td>\n",
              "      <td>0.58984</td>\n",
              "      <td>-0.22145</td>\n",
              "      <td>0.43100</td>\n",
              "      <td>-0.17365</td>\n",
              "      <td>0.60436</td>\n",
              "      <td>-0.24180</td>\n",
              "      <td>0.56045</td>\n",
              "      <td>-0.38238</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.45161</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.71216</td>\n",
              "      <td>-1.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>-1.00000</td>\n",
              "      <td>0.14516</td>\n",
              "      <td>0.54094</td>\n",
              "      <td>-0.39330</td>\n",
              "      <td>-1.00000</td>\n",
              "      <td>-0.54467</td>\n",
              "      <td>-0.69975</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.90695</td>\n",
              "      <td>0.51613</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.20099</td>\n",
              "      <td>0.25682</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.32382</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.02401</td>\n",
              "      <td>0.94140</td>\n",
              "      <td>0.06531</td>\n",
              "      <td>0.92106</td>\n",
              "      <td>-0.23255</td>\n",
              "      <td>0.77152</td>\n",
              "      <td>-0.16399</td>\n",
              "      <td>0.52798</td>\n",
              "      <td>-0.20275</td>\n",
              "      <td>0.56409</td>\n",
              "      <td>-0.00712</td>\n",
              "      <td>0.34395</td>\n",
              "      <td>-0.27457</td>\n",
              "      <td>0.52940</td>\n",
              "      <td>-0.21780</td>\n",
              "      <td>0.45107</td>\n",
              "      <td>-0.17813</td>\n",
              "      <td>0.05982</td>\n",
              "      <td>-0.35575</td>\n",
              "      <td>0.02309</td>\n",
              "      <td>-0.52879</td>\n",
              "      <td>0.03286</td>\n",
              "      <td>-0.65158</td>\n",
              "      <td>0.13290</td>\n",
              "      <td>-0.53206</td>\n",
              "      <td>0.02431</td>\n",
              "      <td>-0.62197</td>\n",
              "      <td>-0.05707</td>\n",
              "      <td>-0.59573</td>\n",
              "      <td>-0.04608</td>\n",
              "      <td>-0.65697</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   feature1  feature2  feature3  ...  feature33  feature34  label\n",
              "0         1         0   0.99539  ...    0.18641   -0.45300      g\n",
              "1         1         0   1.00000  ...   -0.13738   -0.02447      b\n",
              "2         1         0   1.00000  ...    0.56045   -0.38238      g\n",
              "3         1         0   1.00000  ...   -0.32382    1.00000      b\n",
              "4         1         0   1.00000  ...   -0.04608   -0.65697      g\n",
              "\n",
              "[5 rows x 35 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "LfeNWACg3rK8",
        "outputId": "7b81659f-8ca9-4b91-855f-4a830c4d528f"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature1</th>\n",
              "      <th>feature2</th>\n",
              "      <th>feature3</th>\n",
              "      <th>feature4</th>\n",
              "      <th>feature5</th>\n",
              "      <th>feature6</th>\n",
              "      <th>feature7</th>\n",
              "      <th>feature8</th>\n",
              "      <th>feature9</th>\n",
              "      <th>feature10</th>\n",
              "      <th>feature11</th>\n",
              "      <th>feature12</th>\n",
              "      <th>feature13</th>\n",
              "      <th>feature14</th>\n",
              "      <th>feature15</th>\n",
              "      <th>feature16</th>\n",
              "      <th>feature17</th>\n",
              "      <th>feature18</th>\n",
              "      <th>feature19</th>\n",
              "      <th>feature20</th>\n",
              "      <th>feature21</th>\n",
              "      <th>feature22</th>\n",
              "      <th>feature23</th>\n",
              "      <th>feature24</th>\n",
              "      <th>feature25</th>\n",
              "      <th>feature26</th>\n",
              "      <th>feature27</th>\n",
              "      <th>feature28</th>\n",
              "      <th>feature29</th>\n",
              "      <th>feature30</th>\n",
              "      <th>feature31</th>\n",
              "      <th>feature32</th>\n",
              "      <th>feature33</th>\n",
              "      <th>feature34</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.0</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.891738</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.641342</td>\n",
              "      <td>0.044372</td>\n",
              "      <td>0.601068</td>\n",
              "      <td>0.115889</td>\n",
              "      <td>0.550095</td>\n",
              "      <td>0.119360</td>\n",
              "      <td>0.511848</td>\n",
              "      <td>0.181345</td>\n",
              "      <td>0.476183</td>\n",
              "      <td>0.155040</td>\n",
              "      <td>0.400801</td>\n",
              "      <td>0.093414</td>\n",
              "      <td>0.344159</td>\n",
              "      <td>0.071132</td>\n",
              "      <td>0.381949</td>\n",
              "      <td>-0.003617</td>\n",
              "      <td>0.359390</td>\n",
              "      <td>-0.024025</td>\n",
              "      <td>0.336695</td>\n",
              "      <td>0.008296</td>\n",
              "      <td>0.362475</td>\n",
              "      <td>-0.057406</td>\n",
              "      <td>0.396135</td>\n",
              "      <td>-0.071187</td>\n",
              "      <td>0.541641</td>\n",
              "      <td>-0.069538</td>\n",
              "      <td>0.378445</td>\n",
              "      <td>-0.027907</td>\n",
              "      <td>0.352514</td>\n",
              "      <td>-0.003794</td>\n",
              "      <td>0.349364</td>\n",
              "      <td>0.014480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.311155</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.497708</td>\n",
              "      <td>0.441435</td>\n",
              "      <td>0.519862</td>\n",
              "      <td>0.460810</td>\n",
              "      <td>0.492654</td>\n",
              "      <td>0.520750</td>\n",
              "      <td>0.507066</td>\n",
              "      <td>0.483851</td>\n",
              "      <td>0.563496</td>\n",
              "      <td>0.494817</td>\n",
              "      <td>0.622186</td>\n",
              "      <td>0.494873</td>\n",
              "      <td>0.652828</td>\n",
              "      <td>0.458371</td>\n",
              "      <td>0.618020</td>\n",
              "      <td>0.496762</td>\n",
              "      <td>0.626267</td>\n",
              "      <td>0.519076</td>\n",
              "      <td>0.609828</td>\n",
              "      <td>0.518166</td>\n",
              "      <td>0.603767</td>\n",
              "      <td>0.527456</td>\n",
              "      <td>0.578451</td>\n",
              "      <td>0.508495</td>\n",
              "      <td>0.516205</td>\n",
              "      <td>0.550025</td>\n",
              "      <td>0.575886</td>\n",
              "      <td>0.507974</td>\n",
              "      <td>0.571483</td>\n",
              "      <td>0.513574</td>\n",
              "      <td>0.522663</td>\n",
              "      <td>0.468337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.472135</td>\n",
              "      <td>-0.064735</td>\n",
              "      <td>0.412660</td>\n",
              "      <td>-0.024795</td>\n",
              "      <td>0.211310</td>\n",
              "      <td>-0.054840</td>\n",
              "      <td>0.087110</td>\n",
              "      <td>-0.048075</td>\n",
              "      <td>0.021120</td>\n",
              "      <td>-0.065265</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.073725</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.081705</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.225690</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.234670</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.243870</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.366885</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.332390</td>\n",
              "      <td>0.286435</td>\n",
              "      <td>-0.443165</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.236885</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.242595</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.165350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.871110</td>\n",
              "      <td>0.016310</td>\n",
              "      <td>0.809200</td>\n",
              "      <td>0.022800</td>\n",
              "      <td>0.728730</td>\n",
              "      <td>0.014710</td>\n",
              "      <td>0.684210</td>\n",
              "      <td>0.018290</td>\n",
              "      <td>0.667980</td>\n",
              "      <td>0.028250</td>\n",
              "      <td>0.644070</td>\n",
              "      <td>0.030270</td>\n",
              "      <td>0.601940</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.590910</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.576190</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.499090</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.531760</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.553890</td>\n",
              "      <td>-0.015050</td>\n",
              "      <td>0.708240</td>\n",
              "      <td>-0.017690</td>\n",
              "      <td>0.496640</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.442770</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.409560</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.194185</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.334655</td>\n",
              "      <td>0.969240</td>\n",
              "      <td>0.445675</td>\n",
              "      <td>0.953240</td>\n",
              "      <td>0.534195</td>\n",
              "      <td>0.957895</td>\n",
              "      <td>0.482375</td>\n",
              "      <td>0.955505</td>\n",
              "      <td>0.374860</td>\n",
              "      <td>0.919330</td>\n",
              "      <td>0.308975</td>\n",
              "      <td>0.935705</td>\n",
              "      <td>0.195285</td>\n",
              "      <td>0.899265</td>\n",
              "      <td>0.134370</td>\n",
              "      <td>0.894865</td>\n",
              "      <td>0.188760</td>\n",
              "      <td>0.911235</td>\n",
              "      <td>0.164630</td>\n",
              "      <td>0.905240</td>\n",
              "      <td>0.156765</td>\n",
              "      <td>0.999945</td>\n",
              "      <td>0.153535</td>\n",
              "      <td>0.883465</td>\n",
              "      <td>0.154075</td>\n",
              "      <td>0.857620</td>\n",
              "      <td>0.200120</td>\n",
              "      <td>0.813765</td>\n",
              "      <td>0.171660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         feature1  feature2    feature3  ...   feature32   feature33   feature34\n",
              "count  351.000000     351.0  351.000000  ...  351.000000  351.000000  351.000000\n",
              "mean     0.891738       0.0    0.641342  ...   -0.003794    0.349364    0.014480\n",
              "std      0.311155       0.0    0.497708  ...    0.513574    0.522663    0.468337\n",
              "min      0.000000       0.0   -1.000000  ...   -1.000000   -1.000000   -1.000000\n",
              "25%      1.000000       0.0    0.472135  ...   -0.242595    0.000000   -0.165350\n",
              "50%      1.000000       0.0    0.871110  ...    0.000000    0.409560    0.000000\n",
              "75%      1.000000       0.0    1.000000  ...    0.200120    0.813765    0.171660\n",
              "max      1.000000       0.0    1.000000  ...    1.000000    1.000000    1.000000\n",
              "\n",
              "[8 rows x 34 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbZVPTmL3_8X"
      },
      "source": [
        "df.drop(df.columns[1], inplace=True, axis=1)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "o0zp_7qk4Cqg",
        "outputId": "8ab231a6-3da2-4fd7-a981-ee649f8572a8"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature1</th>\n",
              "      <th>feature3</th>\n",
              "      <th>feature4</th>\n",
              "      <th>feature5</th>\n",
              "      <th>feature6</th>\n",
              "      <th>feature7</th>\n",
              "      <th>feature8</th>\n",
              "      <th>feature9</th>\n",
              "      <th>feature10</th>\n",
              "      <th>feature11</th>\n",
              "      <th>feature12</th>\n",
              "      <th>feature13</th>\n",
              "      <th>feature14</th>\n",
              "      <th>feature15</th>\n",
              "      <th>feature16</th>\n",
              "      <th>feature17</th>\n",
              "      <th>feature18</th>\n",
              "      <th>feature19</th>\n",
              "      <th>feature20</th>\n",
              "      <th>feature21</th>\n",
              "      <th>feature22</th>\n",
              "      <th>feature23</th>\n",
              "      <th>feature24</th>\n",
              "      <th>feature25</th>\n",
              "      <th>feature26</th>\n",
              "      <th>feature27</th>\n",
              "      <th>feature28</th>\n",
              "      <th>feature29</th>\n",
              "      <th>feature30</th>\n",
              "      <th>feature31</th>\n",
              "      <th>feature32</th>\n",
              "      <th>feature33</th>\n",
              "      <th>feature34</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.99539</td>\n",
              "      <td>-0.05889</td>\n",
              "      <td>0.85243</td>\n",
              "      <td>0.02306</td>\n",
              "      <td>0.83398</td>\n",
              "      <td>-0.37708</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.03760</td>\n",
              "      <td>0.85243</td>\n",
              "      <td>-0.17755</td>\n",
              "      <td>0.59755</td>\n",
              "      <td>-0.44945</td>\n",
              "      <td>0.60536</td>\n",
              "      <td>-0.38223</td>\n",
              "      <td>0.84356</td>\n",
              "      <td>-0.38542</td>\n",
              "      <td>0.58212</td>\n",
              "      <td>-0.32192</td>\n",
              "      <td>0.56971</td>\n",
              "      <td>-0.29674</td>\n",
              "      <td>0.36946</td>\n",
              "      <td>-0.47357</td>\n",
              "      <td>0.56811</td>\n",
              "      <td>-0.51171</td>\n",
              "      <td>0.41078</td>\n",
              "      <td>-0.46168</td>\n",
              "      <td>0.21266</td>\n",
              "      <td>-0.34090</td>\n",
              "      <td>0.42267</td>\n",
              "      <td>-0.54487</td>\n",
              "      <td>0.18641</td>\n",
              "      <td>-0.45300</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.18829</td>\n",
              "      <td>0.93035</td>\n",
              "      <td>-0.36156</td>\n",
              "      <td>-0.10868</td>\n",
              "      <td>-0.93597</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.04549</td>\n",
              "      <td>0.50874</td>\n",
              "      <td>-0.67743</td>\n",
              "      <td>0.34432</td>\n",
              "      <td>-0.69707</td>\n",
              "      <td>-0.51685</td>\n",
              "      <td>-0.97515</td>\n",
              "      <td>0.05499</td>\n",
              "      <td>-0.62237</td>\n",
              "      <td>0.33109</td>\n",
              "      <td>-1.00000</td>\n",
              "      <td>-0.13151</td>\n",
              "      <td>-0.45300</td>\n",
              "      <td>-0.18056</td>\n",
              "      <td>-0.35734</td>\n",
              "      <td>-0.20332</td>\n",
              "      <td>-0.26569</td>\n",
              "      <td>-0.20468</td>\n",
              "      <td>-0.18401</td>\n",
              "      <td>-0.19040</td>\n",
              "      <td>-0.11593</td>\n",
              "      <td>-0.16626</td>\n",
              "      <td>-0.06288</td>\n",
              "      <td>-0.13738</td>\n",
              "      <td>-0.02447</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.03365</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.00485</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.12062</td>\n",
              "      <td>0.88965</td>\n",
              "      <td>0.01198</td>\n",
              "      <td>0.73082</td>\n",
              "      <td>0.05346</td>\n",
              "      <td>0.85443</td>\n",
              "      <td>0.00827</td>\n",
              "      <td>0.54591</td>\n",
              "      <td>0.00299</td>\n",
              "      <td>0.83775</td>\n",
              "      <td>-0.13644</td>\n",
              "      <td>0.75535</td>\n",
              "      <td>-0.08540</td>\n",
              "      <td>0.70887</td>\n",
              "      <td>-0.27502</td>\n",
              "      <td>0.43385</td>\n",
              "      <td>-0.12062</td>\n",
              "      <td>0.57528</td>\n",
              "      <td>-0.40220</td>\n",
              "      <td>0.58984</td>\n",
              "      <td>-0.22145</td>\n",
              "      <td>0.43100</td>\n",
              "      <td>-0.17365</td>\n",
              "      <td>0.60436</td>\n",
              "      <td>-0.24180</td>\n",
              "      <td>0.56045</td>\n",
              "      <td>-0.38238</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.45161</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.71216</td>\n",
              "      <td>-1.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>-1.00000</td>\n",
              "      <td>0.14516</td>\n",
              "      <td>0.54094</td>\n",
              "      <td>-0.39330</td>\n",
              "      <td>-1.00000</td>\n",
              "      <td>-0.54467</td>\n",
              "      <td>-0.69975</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.90695</td>\n",
              "      <td>0.51613</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.20099</td>\n",
              "      <td>0.25682</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.32382</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.02401</td>\n",
              "      <td>0.94140</td>\n",
              "      <td>0.06531</td>\n",
              "      <td>0.92106</td>\n",
              "      <td>-0.23255</td>\n",
              "      <td>0.77152</td>\n",
              "      <td>-0.16399</td>\n",
              "      <td>0.52798</td>\n",
              "      <td>-0.20275</td>\n",
              "      <td>0.56409</td>\n",
              "      <td>-0.00712</td>\n",
              "      <td>0.34395</td>\n",
              "      <td>-0.27457</td>\n",
              "      <td>0.52940</td>\n",
              "      <td>-0.21780</td>\n",
              "      <td>0.45107</td>\n",
              "      <td>-0.17813</td>\n",
              "      <td>0.05982</td>\n",
              "      <td>-0.35575</td>\n",
              "      <td>0.02309</td>\n",
              "      <td>-0.52879</td>\n",
              "      <td>0.03286</td>\n",
              "      <td>-0.65158</td>\n",
              "      <td>0.13290</td>\n",
              "      <td>-0.53206</td>\n",
              "      <td>0.02431</td>\n",
              "      <td>-0.62197</td>\n",
              "      <td>-0.05707</td>\n",
              "      <td>-0.59573</td>\n",
              "      <td>-0.04608</td>\n",
              "      <td>-0.65697</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   feature1  feature3  feature4  ...  feature33  feature34  label\n",
              "0         1   0.99539  -0.05889  ...    0.18641   -0.45300      g\n",
              "1         1   1.00000  -0.18829  ...   -0.13738   -0.02447      b\n",
              "2         1   1.00000  -0.03365  ...    0.56045   -0.38238      g\n",
              "3         1   1.00000  -0.45161  ...   -0.32382    1.00000      b\n",
              "4         1   1.00000  -0.02401  ...   -0.04608   -0.65697      g\n",
              "\n",
              "[5 rows x 34 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewftaz-G4Hiu",
        "outputId": "f03d061d-79cf-4512-8712-e8ae3e6689ff"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 351 entries, 0 to 350\n",
            "Data columns (total 34 columns):\n",
            " #   Column     Non-Null Count  Dtype  \n",
            "---  ------     --------------  -----  \n",
            " 0   feature1   351 non-null    int64  \n",
            " 1   feature3   351 non-null    float64\n",
            " 2   feature4   351 non-null    float64\n",
            " 3   feature5   351 non-null    float64\n",
            " 4   feature6   351 non-null    float64\n",
            " 5   feature7   351 non-null    float64\n",
            " 6   feature8   351 non-null    float64\n",
            " 7   feature9   351 non-null    float64\n",
            " 8   feature10  351 non-null    float64\n",
            " 9   feature11  351 non-null    float64\n",
            " 10  feature12  351 non-null    float64\n",
            " 11  feature13  351 non-null    float64\n",
            " 12  feature14  351 non-null    float64\n",
            " 13  feature15  351 non-null    float64\n",
            " 14  feature16  351 non-null    float64\n",
            " 15  feature17  351 non-null    float64\n",
            " 16  feature18  351 non-null    float64\n",
            " 17  feature19  351 non-null    float64\n",
            " 18  feature20  351 non-null    float64\n",
            " 19  feature21  351 non-null    float64\n",
            " 20  feature22  351 non-null    float64\n",
            " 21  feature23  351 non-null    float64\n",
            " 22  feature24  351 non-null    float64\n",
            " 23  feature25  351 non-null    float64\n",
            " 24  feature26  351 non-null    float64\n",
            " 25  feature27  351 non-null    float64\n",
            " 26  feature28  351 non-null    float64\n",
            " 27  feature29  351 non-null    float64\n",
            " 28  feature30  351 non-null    float64\n",
            " 29  feature31  351 non-null    float64\n",
            " 30  feature32  351 non-null    float64\n",
            " 31  feature33  351 non-null    float64\n",
            " 32  feature34  351 non-null    float64\n",
            " 33  label      351 non-null    object \n",
            "dtypes: float64(32), int64(1), object(1)\n",
            "memory usage: 93.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKhvvPff4e90",
        "outputId": "390ee0b8-f9d4-443c-db8a-06376fa27433"
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "feature1     0\n",
              "feature3     0\n",
              "feature4     0\n",
              "feature5     0\n",
              "feature6     0\n",
              "feature7     0\n",
              "feature8     0\n",
              "feature9     0\n",
              "feature10    0\n",
              "feature11    0\n",
              "feature12    0\n",
              "feature13    0\n",
              "feature14    0\n",
              "feature15    0\n",
              "feature16    0\n",
              "feature17    0\n",
              "feature18    0\n",
              "feature19    0\n",
              "feature20    0\n",
              "feature21    0\n",
              "feature22    0\n",
              "feature23    0\n",
              "feature24    0\n",
              "feature25    0\n",
              "feature26    0\n",
              "feature27    0\n",
              "feature28    0\n",
              "feature29    0\n",
              "feature30    0\n",
              "feature31    0\n",
              "feature32    0\n",
              "feature33    0\n",
              "feature34    0\n",
              "label        0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWO8zNlc5HWR"
      },
      "source": [
        "df['label'] = [1 if lbl == 'g' else 0 for lbl in df['label']]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vZstwEf5KL0"
      },
      "source": [
        "np.random.seed(11111)\n",
        "msk = np.random.rand(len(df)) <= 0.50\n",
        "train_data = df[msk]\n",
        "temp = df[~msk]\n",
        "msk1 = np.random.rand(len(temp)) <= 0.60\n",
        "test_data = temp[msk1]\n",
        "val_data = temp[~msk1]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oO6a7I3B6Kib"
      },
      "source": [
        "train_label = train_data.iloc[:,-1]\n",
        "train_data = train_data.iloc[:,0:-1]\n",
        "test_label = test_data.iloc[:,-1]\n",
        "test_data = test_data.iloc[:,0:-1]\n",
        "val_label = val_data.iloc[:,-1]\n",
        "val_data = val_data.iloc[:,0:-1]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "rRTrw6kA6qvk",
        "outputId": "02784742-4adb-4cbe-d545-b83827126d59"
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature1</th>\n",
              "      <th>feature3</th>\n",
              "      <th>feature4</th>\n",
              "      <th>feature5</th>\n",
              "      <th>feature6</th>\n",
              "      <th>feature7</th>\n",
              "      <th>feature8</th>\n",
              "      <th>feature9</th>\n",
              "      <th>feature10</th>\n",
              "      <th>feature11</th>\n",
              "      <th>feature12</th>\n",
              "      <th>feature13</th>\n",
              "      <th>feature14</th>\n",
              "      <th>feature15</th>\n",
              "      <th>feature16</th>\n",
              "      <th>feature17</th>\n",
              "      <th>feature18</th>\n",
              "      <th>feature19</th>\n",
              "      <th>feature20</th>\n",
              "      <th>feature21</th>\n",
              "      <th>feature22</th>\n",
              "      <th>feature23</th>\n",
              "      <th>feature24</th>\n",
              "      <th>feature25</th>\n",
              "      <th>feature26</th>\n",
              "      <th>feature27</th>\n",
              "      <th>feature28</th>\n",
              "      <th>feature29</th>\n",
              "      <th>feature30</th>\n",
              "      <th>feature31</th>\n",
              "      <th>feature32</th>\n",
              "      <th>feature33</th>\n",
              "      <th>feature34</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.18829</td>\n",
              "      <td>0.93035</td>\n",
              "      <td>-0.36156</td>\n",
              "      <td>-0.10868</td>\n",
              "      <td>-0.93597</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.04549</td>\n",
              "      <td>0.50874</td>\n",
              "      <td>-0.67743</td>\n",
              "      <td>0.34432</td>\n",
              "      <td>-0.69707</td>\n",
              "      <td>-0.51685</td>\n",
              "      <td>-0.97515</td>\n",
              "      <td>0.05499</td>\n",
              "      <td>-0.62237</td>\n",
              "      <td>0.33109</td>\n",
              "      <td>-1.00000</td>\n",
              "      <td>-0.13151</td>\n",
              "      <td>-0.45300</td>\n",
              "      <td>-0.18056</td>\n",
              "      <td>-0.35734</td>\n",
              "      <td>-0.20332</td>\n",
              "      <td>-0.26569</td>\n",
              "      <td>-0.20468</td>\n",
              "      <td>-0.18401</td>\n",
              "      <td>-0.19040</td>\n",
              "      <td>-0.11593</td>\n",
              "      <td>-0.16626</td>\n",
              "      <td>-0.06288</td>\n",
              "      <td>-0.13738</td>\n",
              "      <td>-0.02447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.03365</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.00485</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.12062</td>\n",
              "      <td>0.88965</td>\n",
              "      <td>0.01198</td>\n",
              "      <td>0.73082</td>\n",
              "      <td>0.05346</td>\n",
              "      <td>0.85443</td>\n",
              "      <td>0.00827</td>\n",
              "      <td>0.54591</td>\n",
              "      <td>0.00299</td>\n",
              "      <td>0.83775</td>\n",
              "      <td>-0.13644</td>\n",
              "      <td>0.75535</td>\n",
              "      <td>-0.08540</td>\n",
              "      <td>0.70887</td>\n",
              "      <td>-0.27502</td>\n",
              "      <td>0.43385</td>\n",
              "      <td>-0.12062</td>\n",
              "      <td>0.57528</td>\n",
              "      <td>-0.40220</td>\n",
              "      <td>0.58984</td>\n",
              "      <td>-0.22145</td>\n",
              "      <td>0.43100</td>\n",
              "      <td>-0.17365</td>\n",
              "      <td>0.60436</td>\n",
              "      <td>-0.24180</td>\n",
              "      <td>0.56045</td>\n",
              "      <td>-0.38238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.45161</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.71216</td>\n",
              "      <td>-1.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>-1.00000</td>\n",
              "      <td>0.14516</td>\n",
              "      <td>0.54094</td>\n",
              "      <td>-0.39330</td>\n",
              "      <td>-1.00000</td>\n",
              "      <td>-0.54467</td>\n",
              "      <td>-0.69975</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.90695</td>\n",
              "      <td>0.51613</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.20099</td>\n",
              "      <td>0.25682</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.32382</td>\n",
              "      <td>1.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.02401</td>\n",
              "      <td>0.94140</td>\n",
              "      <td>0.06531</td>\n",
              "      <td>0.92106</td>\n",
              "      <td>-0.23255</td>\n",
              "      <td>0.77152</td>\n",
              "      <td>-0.16399</td>\n",
              "      <td>0.52798</td>\n",
              "      <td>-0.20275</td>\n",
              "      <td>0.56409</td>\n",
              "      <td>-0.00712</td>\n",
              "      <td>0.34395</td>\n",
              "      <td>-0.27457</td>\n",
              "      <td>0.52940</td>\n",
              "      <td>-0.21780</td>\n",
              "      <td>0.45107</td>\n",
              "      <td>-0.17813</td>\n",
              "      <td>0.05982</td>\n",
              "      <td>-0.35575</td>\n",
              "      <td>0.02309</td>\n",
              "      <td>-0.52879</td>\n",
              "      <td>0.03286</td>\n",
              "      <td>-0.65158</td>\n",
              "      <td>0.13290</td>\n",
              "      <td>-0.53206</td>\n",
              "      <td>0.02431</td>\n",
              "      <td>-0.62197</td>\n",
              "      <td>-0.05707</td>\n",
              "      <td>-0.59573</td>\n",
              "      <td>-0.04608</td>\n",
              "      <td>-0.65697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>0.02337</td>\n",
              "      <td>-0.00592</td>\n",
              "      <td>-0.09924</td>\n",
              "      <td>-0.11949</td>\n",
              "      <td>-0.00763</td>\n",
              "      <td>-0.11824</td>\n",
              "      <td>0.14706</td>\n",
              "      <td>0.06637</td>\n",
              "      <td>0.03786</td>\n",
              "      <td>-0.06302</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>-0.04572</td>\n",
              "      <td>-0.15540</td>\n",
              "      <td>-0.00343</td>\n",
              "      <td>-0.10196</td>\n",
              "      <td>-0.11575</td>\n",
              "      <td>-0.05414</td>\n",
              "      <td>0.01838</td>\n",
              "      <td>0.03669</td>\n",
              "      <td>0.01519</td>\n",
              "      <td>0.00888</td>\n",
              "      <td>0.03513</td>\n",
              "      <td>-0.01535</td>\n",
              "      <td>-0.03240</td>\n",
              "      <td>0.09223</td>\n",
              "      <td>-0.07859</td>\n",
              "      <td>0.00732</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>-0.00039</td>\n",
              "      <td>0.12011</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   feature1  feature3  feature4  ...  feature32  feature33  feature34\n",
              "1         1   1.00000  -0.18829  ...   -0.06288   -0.13738   -0.02447\n",
              "2         1   1.00000  -0.03365  ...   -0.24180    0.56045   -0.38238\n",
              "3         1   1.00000  -0.45161  ...    1.00000   -0.32382    1.00000\n",
              "4         1   1.00000  -0.02401  ...   -0.59573   -0.04608   -0.65697\n",
              "5         1   0.02337  -0.00592  ...    0.00000   -0.00039    0.12011\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnKqYgTm6TyB",
        "outputId": "938303c7-359e-4e1e-f027-0eeac126eda5"
      },
      "source": [
        "train_label"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1      0\n",
              "2      1\n",
              "3      0\n",
              "4      1\n",
              "5      0\n",
              "      ..\n",
              "343    1\n",
              "344    1\n",
              "346    1\n",
              "348    1\n",
              "349    1\n",
              "Name: label, Length: 175, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBE6JUDn7Ibw"
      },
      "source": [
        "# # Normalize the data\n",
        "train_mean = train_data.mean()\n",
        "train_data -= train_mean\n",
        "train_std = train_data.std()\n",
        "train_data /= train_std\n",
        "test_data -= train_mean\n",
        "test_data /= train_std"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8oxCBvl7W83",
        "outputId": "1a0d85cb-1000-44b0-88c1-747fe0fbf727"
      },
      "source": [
        "train_data.shape\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(175, 33)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BACca9TR7jsm",
        "outputId": "0d7ac48e-001e-499e-cf75-8c56d3ead20c"
      },
      "source": [
        "test_data.shape\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(109, 33)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmfNSmt17lsw",
        "outputId": "79196015-c713-4e27-ea01-372d336e0822"
      },
      "source": [
        "val_data.shape"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(67, 33)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E76Daf_7zdo"
      },
      "source": [
        "train_data = train_data.to_numpy()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUNw8bO577Fw"
      },
      "source": [
        "train_label = train_label.to_numpy().astype('float32')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jE4Kgxfx8Dcv"
      },
      "source": [
        "test_data = test_data.to_numpy()"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWDYUC4m8Hri"
      },
      "source": [
        "test_label = test_label.to_numpy().astype('float32')"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNwvZ6Xk8J2h"
      },
      "source": [
        "val_data = val_data.to_numpy()"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMdn2U9-8QJG"
      },
      "source": [
        "val_label = val_label.to_numpy().astype('float32')"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbu6Yr7m8fOs"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjTZ1hn1-kJ_"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(10, activation='relu', input_shape=(train_data.shape[1],)))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(6, activation='relu'))\n",
        "model.add(Dense(1,  activation='sigmoid'))"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfH9hdZU_BV2"
      },
      "source": [
        "model.compile(optimizer = 'RMSprop', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWrLJDJw_PVJ",
        "outputId": "234090db-e246-49b4-d224-978b8f067306"
      },
      "source": [
        "history = model.fit(train_data, train_label, validation_data=(val_data,val_label), epochs=100, batch_size = 16)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "11/11 [==============================] - 1s 26ms/step - loss: 0.6762 - accuracy: 0.6554 - val_loss: 0.6822 - val_accuracy: 0.5821\n",
            "Epoch 2/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.6584 - accuracy: 0.6853 - val_loss: 0.6746 - val_accuracy: 0.5821\n",
            "Epoch 3/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.6428 - accuracy: 0.6422 - val_loss: 0.6699 - val_accuracy: 0.5821\n",
            "Epoch 4/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.6202 - accuracy: 0.6333 - val_loss: 0.6677 - val_accuracy: 0.5821\n",
            "Epoch 5/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.5900 - accuracy: 0.6945 - val_loss: 0.6682 - val_accuracy: 0.5821\n",
            "Epoch 6/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.5798 - accuracy: 0.7084 - val_loss: 0.6706 - val_accuracy: 0.5821\n",
            "Epoch 7/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.5605 - accuracy: 0.6768 - val_loss: 0.6741 - val_accuracy: 0.5821\n",
            "Epoch 8/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.5462 - accuracy: 0.6963 - val_loss: 0.6790 - val_accuracy: 0.5821\n",
            "Epoch 9/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.5999 - accuracy: 0.6296 - val_loss: 0.6855 - val_accuracy: 0.5821\n",
            "Epoch 10/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.5324 - accuracy: 0.7035 - val_loss: 0.6904 - val_accuracy: 0.5821\n",
            "Epoch 11/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.4989 - accuracy: 0.7241 - val_loss: 0.6951 - val_accuracy: 0.5821\n",
            "Epoch 12/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.5083 - accuracy: 0.6939 - val_loss: 0.7006 - val_accuracy: 0.5821\n",
            "Epoch 13/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.5082 - accuracy: 0.7301 - val_loss: 0.7067 - val_accuracy: 0.5821\n",
            "Epoch 14/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.5246 - accuracy: 0.7172 - val_loss: 0.7128 - val_accuracy: 0.5821\n",
            "Epoch 15/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.4954 - accuracy: 0.7072 - val_loss: 0.7161 - val_accuracy: 0.5821\n",
            "Epoch 16/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.4452 - accuracy: 0.7533 - val_loss: 0.7176 - val_accuracy: 0.5821\n",
            "Epoch 17/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.3999 - accuracy: 0.7890 - val_loss: 0.7195 - val_accuracy: 0.5821\n",
            "Epoch 18/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.4453 - accuracy: 0.7587 - val_loss: 0.7234 - val_accuracy: 0.5821\n",
            "Epoch 19/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.4083 - accuracy: 0.7835 - val_loss: 0.7268 - val_accuracy: 0.5821\n",
            "Epoch 20/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.4562 - accuracy: 0.7588 - val_loss: 0.7331 - val_accuracy: 0.5821\n",
            "Epoch 21/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.4040 - accuracy: 0.8249 - val_loss: 0.7339 - val_accuracy: 0.5821\n",
            "Epoch 22/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.4108 - accuracy: 0.8355 - val_loss: 0.7383 - val_accuracy: 0.5821\n",
            "Epoch 23/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.4103 - accuracy: 0.8370 - val_loss: 0.7418 - val_accuracy: 0.5821\n",
            "Epoch 24/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.4034 - accuracy: 0.8607 - val_loss: 0.7418 - val_accuracy: 0.5821\n",
            "Epoch 25/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.3416 - accuracy: 0.8932 - val_loss: 0.7407 - val_accuracy: 0.5821\n",
            "Epoch 26/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.4182 - accuracy: 0.8435 - val_loss: 0.7469 - val_accuracy: 0.5970\n",
            "Epoch 27/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.3977 - accuracy: 0.8703 - val_loss: 0.7511 - val_accuracy: 0.5970\n",
            "Epoch 28/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.3920 - accuracy: 0.8886 - val_loss: 0.7582 - val_accuracy: 0.5970\n",
            "Epoch 29/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.3499 - accuracy: 0.9072 - val_loss: 0.7577 - val_accuracy: 0.5970\n",
            "Epoch 30/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.3443 - accuracy: 0.9268 - val_loss: 0.7608 - val_accuracy: 0.5970\n",
            "Epoch 31/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.3306 - accuracy: 0.9197 - val_loss: 0.7645 - val_accuracy: 0.5970\n",
            "Epoch 32/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.3437 - accuracy: 0.9056 - val_loss: 0.7751 - val_accuracy: 0.5970\n",
            "Epoch 33/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.3160 - accuracy: 0.9063 - val_loss: 0.7782 - val_accuracy: 0.5970\n",
            "Epoch 34/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.3241 - accuracy: 0.8872 - val_loss: 0.7819 - val_accuracy: 0.6119\n",
            "Epoch 35/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2692 - accuracy: 0.9454 - val_loss: 0.7854 - val_accuracy: 0.6119\n",
            "Epoch 36/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.3003 - accuracy: 0.9077 - val_loss: 0.7933 - val_accuracy: 0.6119\n",
            "Epoch 37/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2531 - accuracy: 0.9390 - val_loss: 0.7956 - val_accuracy: 0.6119\n",
            "Epoch 38/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2377 - accuracy: 0.9358 - val_loss: 0.8024 - val_accuracy: 0.6119\n",
            "Epoch 39/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2756 - accuracy: 0.9076 - val_loss: 0.8169 - val_accuracy: 0.6269\n",
            "Epoch 40/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2460 - accuracy: 0.9203 - val_loss: 0.8229 - val_accuracy: 0.6269\n",
            "Epoch 41/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2239 - accuracy: 0.9519 - val_loss: 0.8304 - val_accuracy: 0.6269\n",
            "Epoch 42/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2115 - accuracy: 0.9397 - val_loss: 0.8285 - val_accuracy: 0.6269\n",
            "Epoch 43/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1930 - accuracy: 0.9560 - val_loss: 0.8304 - val_accuracy: 0.6269\n",
            "Epoch 44/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2358 - accuracy: 0.9411 - val_loss: 0.8458 - val_accuracy: 0.6269\n",
            "Epoch 45/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1969 - accuracy: 0.9529 - val_loss: 0.8652 - val_accuracy: 0.6269\n",
            "Epoch 46/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1779 - accuracy: 0.9466 - val_loss: 0.8648 - val_accuracy: 0.6269\n",
            "Epoch 47/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2109 - accuracy: 0.9475 - val_loss: 0.8889 - val_accuracy: 0.6269\n",
            "Epoch 48/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1806 - accuracy: 0.9413 - val_loss: 0.9023 - val_accuracy: 0.6269\n",
            "Epoch 49/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1684 - accuracy: 0.9610 - val_loss: 0.9075 - val_accuracy: 0.6269\n",
            "Epoch 50/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1867 - accuracy: 0.9382 - val_loss: 0.9314 - val_accuracy: 0.6269\n",
            "Epoch 51/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1543 - accuracy: 0.9403 - val_loss: 0.9536 - val_accuracy: 0.6269\n",
            "Epoch 52/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1712 - accuracy: 0.9599 - val_loss: 0.9673 - val_accuracy: 0.6269\n",
            "Epoch 53/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1466 - accuracy: 0.9702 - val_loss: 0.9851 - val_accuracy: 0.6269\n",
            "Epoch 54/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1723 - accuracy: 0.9532 - val_loss: 1.0005 - val_accuracy: 0.6269\n",
            "Epoch 55/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1704 - accuracy: 0.9584 - val_loss: 1.0318 - val_accuracy: 0.6269\n",
            "Epoch 56/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1581 - accuracy: 0.9647 - val_loss: 1.0458 - val_accuracy: 0.6269\n",
            "Epoch 57/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1094 - accuracy: 0.9846 - val_loss: 1.0682 - val_accuracy: 0.6269\n",
            "Epoch 58/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1241 - accuracy: 0.9840 - val_loss: 1.0834 - val_accuracy: 0.6269\n",
            "Epoch 59/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0989 - accuracy: 0.9860 - val_loss: 1.0964 - val_accuracy: 0.6269\n",
            "Epoch 60/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.1057 - accuracy: 0.9805 - val_loss: 1.1210 - val_accuracy: 0.6269\n",
            "Epoch 61/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1371 - accuracy: 0.9747 - val_loss: 1.1521 - val_accuracy: 0.6269\n",
            "Epoch 62/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1016 - accuracy: 0.9745 - val_loss: 1.1795 - val_accuracy: 0.6269\n",
            "Epoch 63/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1133 - accuracy: 0.9756 - val_loss: 1.1979 - val_accuracy: 0.6269\n",
            "Epoch 64/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1022 - accuracy: 0.9782 - val_loss: 1.2215 - val_accuracy: 0.6269\n",
            "Epoch 65/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0949 - accuracy: 0.9813 - val_loss: 1.2351 - val_accuracy: 0.6269\n",
            "Epoch 66/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0873 - accuracy: 0.9857 - val_loss: 1.2607 - val_accuracy: 0.6269\n",
            "Epoch 67/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1096 - accuracy: 0.9797 - val_loss: 1.2839 - val_accuracy: 0.6269\n",
            "Epoch 68/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1040 - accuracy: 0.9698 - val_loss: 1.3060 - val_accuracy: 0.6269\n",
            "Epoch 69/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1116 - accuracy: 0.9735 - val_loss: 1.3239 - val_accuracy: 0.6269\n",
            "Epoch 70/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0748 - accuracy: 0.9924 - val_loss: 1.3414 - val_accuracy: 0.6269\n",
            "Epoch 71/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0765 - accuracy: 0.9873 - val_loss: 1.3565 - val_accuracy: 0.6119\n",
            "Epoch 72/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0992 - accuracy: 0.9696 - val_loss: 1.3834 - val_accuracy: 0.6119\n",
            "Epoch 73/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0994 - accuracy: 0.9789 - val_loss: 1.3953 - val_accuracy: 0.6119\n",
            "Epoch 74/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0809 - accuracy: 0.9829 - val_loss: 1.4017 - val_accuracy: 0.6269\n",
            "Epoch 75/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1050 - accuracy: 0.9787 - val_loss: 1.4222 - val_accuracy: 0.6269\n",
            "Epoch 76/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0882 - accuracy: 0.9831 - val_loss: 1.4500 - val_accuracy: 0.6269\n",
            "Epoch 77/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0680 - accuracy: 0.9819 - val_loss: 1.4680 - val_accuracy: 0.6269\n",
            "Epoch 78/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0886 - accuracy: 0.9860 - val_loss: 1.4948 - val_accuracy: 0.6269\n",
            "Epoch 79/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0934 - accuracy: 0.9666 - val_loss: 1.5228 - val_accuracy: 0.6269\n",
            "Epoch 80/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0481 - accuracy: 0.9915 - val_loss: 1.5305 - val_accuracy: 0.6418\n",
            "Epoch 81/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0761 - accuracy: 0.9847 - val_loss: 1.5677 - val_accuracy: 0.6418\n",
            "Epoch 82/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0874 - accuracy: 0.9707 - val_loss: 1.6009 - val_accuracy: 0.6418\n",
            "Epoch 83/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0560 - accuracy: 0.9836 - val_loss: 1.5921 - val_accuracy: 0.6418\n",
            "Epoch 84/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0630 - accuracy: 0.9783 - val_loss: 1.6314 - val_accuracy: 0.6418\n",
            "Epoch 85/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0406 - accuracy: 0.9938 - val_loss: 1.6369 - val_accuracy: 0.6418\n",
            "Epoch 86/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0569 - accuracy: 0.9827 - val_loss: 1.6590 - val_accuracy: 0.6418\n",
            "Epoch 87/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0755 - accuracy: 0.9790 - val_loss: 1.6784 - val_accuracy: 0.6418\n",
            "Epoch 88/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0495 - accuracy: 0.9905 - val_loss: 1.6997 - val_accuracy: 0.6418\n",
            "Epoch 89/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0454 - accuracy: 0.9899 - val_loss: 1.7343 - val_accuracy: 0.6418\n",
            "Epoch 90/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0539 - accuracy: 0.9809 - val_loss: 1.7622 - val_accuracy: 0.6418\n",
            "Epoch 91/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0863 - accuracy: 0.9644 - val_loss: 1.8078 - val_accuracy: 0.6418\n",
            "Epoch 92/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0836 - accuracy: 0.9592 - val_loss: 1.8603 - val_accuracy: 0.6418\n",
            "Epoch 93/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0366 - accuracy: 0.9860 - val_loss: 1.8456 - val_accuracy: 0.6418\n",
            "Epoch 94/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0340 - accuracy: 0.9922 - val_loss: 1.8639 - val_accuracy: 0.6418\n",
            "Epoch 95/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0447 - accuracy: 0.9739 - val_loss: 1.8979 - val_accuracy: 0.6418\n",
            "Epoch 96/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0332 - accuracy: 0.9891 - val_loss: 1.9364 - val_accuracy: 0.6418\n",
            "Epoch 97/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0645 - accuracy: 0.9701 - val_loss: 1.9769 - val_accuracy: 0.6418\n",
            "Epoch 98/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0335 - accuracy: 0.9922 - val_loss: 2.0097 - val_accuracy: 0.6418\n",
            "Epoch 99/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0432 - accuracy: 0.9901 - val_loss: 2.0424 - val_accuracy: 0.6418\n",
            "Epoch 100/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0528 - accuracy: 0.9856 - val_loss: 2.0726 - val_accuracy: 0.6418\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRskOCNFAPDE"
      },
      "source": [
        "model_1 = Sequential()\n",
        "model_1.add(Dense(64, activation='relu', input_shape=(train_data.shape[1],)))\n",
        "model_1.add(Dense(128, activation='relu'))\n",
        "model_1.add(Dropout(0.2))\n",
        "model_1.add(Dense(128, activation='relu'))\n",
        "model_1.add(Dropout(0.2))\n",
        "model_1.add(Dense(256, activation='relu'))\n",
        "\n",
        "model_1.add(Dense(1,  activation='sigmoid'))"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D8n1pd3Ai16"
      },
      "source": [
        "model_1.compile(optimizer = 'RMSprop', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PJMD9f2AsC7",
        "outputId": "28b0bf2c-afeb-4c82-9a37-a6e2de628d1c"
      },
      "source": [
        "history_1 = model_1.fit(train_data, train_label, validation_data=(val_data,val_label), epochs=100, batch_size = 16)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "11/11 [==============================] - 1s 24ms/step - loss: 0.6393 - accuracy: 0.5932 - val_loss: 0.6956 - val_accuracy: 0.6418\n",
            "Epoch 2/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.2671 - accuracy: 0.9184 - val_loss: 0.5828 - val_accuracy: 0.7164\n",
            "Epoch 3/100\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.2065 - accuracy: 0.9330 - val_loss: 0.8986 - val_accuracy: 0.6567\n",
            "Epoch 4/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.1832 - accuracy: 0.9254 - val_loss: 1.0535 - val_accuracy: 0.6567\n",
            "Epoch 5/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0967 - accuracy: 0.9868 - val_loss: 1.0247 - val_accuracy: 0.6866\n",
            "Epoch 6/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0967 - accuracy: 0.9668 - val_loss: 1.3552 - val_accuracy: 0.6269\n",
            "Epoch 7/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0824 - accuracy: 0.9742 - val_loss: 1.1536 - val_accuracy: 0.6716\n",
            "Epoch 8/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0447 - accuracy: 0.9915 - val_loss: 1.3040 - val_accuracy: 0.6716\n",
            "Epoch 9/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0815 - accuracy: 0.9614 - val_loss: 1.3033 - val_accuracy: 0.6716\n",
            "Epoch 10/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0570 - accuracy: 0.9796 - val_loss: 1.6179 - val_accuracy: 0.6567\n",
            "Epoch 11/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0694 - accuracy: 0.9808 - val_loss: 1.2612 - val_accuracy: 0.6866\n",
            "Epoch 12/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0399 - accuracy: 0.9890 - val_loss: 2.2373 - val_accuracy: 0.6567\n",
            "Epoch 13/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0159 - accuracy: 0.9990 - val_loss: 1.8167 - val_accuracy: 0.6567\n",
            "Epoch 14/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0185 - accuracy: 0.9973 - val_loss: 2.2426 - val_accuracy: 0.6567\n",
            "Epoch 15/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0133 - accuracy: 0.9979 - val_loss: 2.6570 - val_accuracy: 0.6567\n",
            "Epoch 16/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0155 - accuracy: 0.9937 - val_loss: 2.5497 - val_accuracy: 0.6567\n",
            "Epoch 17/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0112 - accuracy: 0.9946 - val_loss: 3.0486 - val_accuracy: 0.6567\n",
            "Epoch 18/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 2.8960 - val_accuracy: 0.6567\n",
            "Epoch 19/100\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.0156 - accuracy: 0.9890 - val_loss: 3.6049 - val_accuracy: 0.6567\n",
            "Epoch 20/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 4.2571 - val_accuracy: 0.6418\n",
            "Epoch 21/100\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 4.5565 - val_accuracy: 0.6418\n",
            "Epoch 22/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 4.4243 - val_accuracy: 0.6567\n",
            "Epoch 23/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 7.2429e-04 - accuracy: 1.0000 - val_loss: 4.4573 - val_accuracy: 0.6567\n",
            "Epoch 24/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 7.7700e-04 - accuracy: 1.0000 - val_loss: 4.9400 - val_accuracy: 0.6567\n",
            "Epoch 25/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.1348e-04 - accuracy: 1.0000 - val_loss: 5.1871 - val_accuracy: 0.6567\n",
            "Epoch 26/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.1263e-04 - accuracy: 1.0000 - val_loss: 5.3469 - val_accuracy: 0.6567\n",
            "Epoch 27/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0114 - accuracy: 0.9916 - val_loss: 6.3680 - val_accuracy: 0.6418\n",
            "Epoch 28/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 5.7708 - val_accuracy: 0.6567\n",
            "Epoch 29/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 8.5990e-04 - accuracy: 1.0000 - val_loss: 5.3643 - val_accuracy: 0.6567\n",
            "Epoch 30/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.5724e-05 - accuracy: 1.0000 - val_loss: 5.7315 - val_accuracy: 0.6567\n",
            "Epoch 31/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.5134e-04 - accuracy: 1.0000 - val_loss: 6.4725 - val_accuracy: 0.6567\n",
            "Epoch 32/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3963e-04 - accuracy: 1.0000 - val_loss: 6.1488 - val_accuracy: 0.6567\n",
            "Epoch 33/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 3.4444e-05 - accuracy: 1.0000 - val_loss: 6.6014 - val_accuracy: 0.6567\n",
            "Epoch 34/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 6.2241 - val_accuracy: 0.6567\n",
            "Epoch 35/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.4370e-05 - accuracy: 1.0000 - val_loss: 6.2696 - val_accuracy: 0.6567\n",
            "Epoch 36/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 2.1888e-05 - accuracy: 1.0000 - val_loss: 7.1095 - val_accuracy: 0.6567\n",
            "Epoch 37/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 3.7791e-05 - accuracy: 1.0000 - val_loss: 6.4619 - val_accuracy: 0.6567\n",
            "Epoch 38/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.9423e-05 - accuracy: 1.0000 - val_loss: 7.2003 - val_accuracy: 0.6567\n",
            "Epoch 39/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.3070e-04 - accuracy: 1.0000 - val_loss: 5.9592 - val_accuracy: 0.6567\n",
            "Epoch 40/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 7.1711 - val_accuracy: 0.6567\n",
            "Epoch 41/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 2.2131e-05 - accuracy: 1.0000 - val_loss: 7.3918 - val_accuracy: 0.6567\n",
            "Epoch 42/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 2.5570e-04 - accuracy: 1.0000 - val_loss: 6.3996 - val_accuracy: 0.6567\n",
            "Epoch 43/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 6.6139e-06 - accuracy: 1.0000 - val_loss: 6.5707 - val_accuracy: 0.6567\n",
            "Epoch 44/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 5.0997e-06 - accuracy: 1.0000 - val_loss: 6.8896 - val_accuracy: 0.6567\n",
            "Epoch 45/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 2.0805e-04 - accuracy: 1.0000 - val_loss: 7.1492 - val_accuracy: 0.6567\n",
            "Epoch 46/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 2.3943e-05 - accuracy: 1.0000 - val_loss: 7.1650 - val_accuracy: 0.6567\n",
            "Epoch 47/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 4.0066e-06 - accuracy: 1.0000 - val_loss: 7.1482 - val_accuracy: 0.6567\n",
            "Epoch 48/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 4.9541e-06 - accuracy: 1.0000 - val_loss: 7.0023 - val_accuracy: 0.6567\n",
            "Epoch 49/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 6.3095e-06 - accuracy: 1.0000 - val_loss: 8.3169 - val_accuracy: 0.6567\n",
            "Epoch 50/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 5.7128e-06 - accuracy: 1.0000 - val_loss: 8.0413 - val_accuracy: 0.6567\n",
            "Epoch 51/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 4.7582e-06 - accuracy: 1.0000 - val_loss: 9.2579 - val_accuracy: 0.6567\n",
            "Epoch 52/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.5257e-05 - accuracy: 1.0000 - val_loss: 8.4964 - val_accuracy: 0.6567\n",
            "Epoch 53/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 4.2291e-07 - accuracy: 1.0000 - val_loss: 8.5978 - val_accuracy: 0.6567\n",
            "Epoch 54/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 4.7948e-06 - accuracy: 1.0000 - val_loss: 10.7545 - val_accuracy: 0.6567\n",
            "Epoch 55/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 4.6553e-06 - accuracy: 1.0000 - val_loss: 9.9563 - val_accuracy: 0.6567\n",
            "Epoch 56/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.8892e-07 - accuracy: 1.0000 - val_loss: 9.9224 - val_accuracy: 0.6567\n",
            "Epoch 57/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 8.0225e-07 - accuracy: 1.0000 - val_loss: 9.6405 - val_accuracy: 0.6567\n",
            "Epoch 58/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 2.1879e-04 - accuracy: 1.0000 - val_loss: 9.5401 - val_accuracy: 0.6567\n",
            "Epoch 59/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 2.1933e-06 - accuracy: 1.0000 - val_loss: 10.3062 - val_accuracy: 0.6567\n",
            "Epoch 60/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 4.0018e-08 - accuracy: 1.0000 - val_loss: 10.3560 - val_accuracy: 0.6567\n",
            "Epoch 61/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 2.2995e-06 - accuracy: 1.0000 - val_loss: 8.5795 - val_accuracy: 0.6567\n",
            "Epoch 62/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 4.0719e-06 - accuracy: 1.0000 - val_loss: 9.6297 - val_accuracy: 0.6567\n",
            "Epoch 63/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.9435e-08 - accuracy: 1.0000 - val_loss: 9.6571 - val_accuracy: 0.6567\n",
            "Epoch 64/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.4444e-08 - accuracy: 1.0000 - val_loss: 10.1920 - val_accuracy: 0.6567\n",
            "Epoch 65/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 8.3155e-08 - accuracy: 1.0000 - val_loss: 10.3356 - val_accuracy: 0.6418\n",
            "Epoch 66/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0076 - accuracy: 0.9979 - val_loss: 10.9970 - val_accuracy: 0.6418\n",
            "Epoch 67/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.1821e-04 - accuracy: 1.0000 - val_loss: 9.6777 - val_accuracy: 0.6567\n",
            "Epoch 68/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.0390e-05 - accuracy: 1.0000 - val_loss: 9.2860 - val_accuracy: 0.6567\n",
            "Epoch 69/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 8.7961e-08 - accuracy: 1.0000 - val_loss: 9.3007 - val_accuracy: 0.6567\n",
            "Epoch 70/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 2.4165e-07 - accuracy: 1.0000 - val_loss: 9.2627 - val_accuracy: 0.6567\n",
            "Epoch 71/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.9771e-07 - accuracy: 1.0000 - val_loss: 9.2139 - val_accuracy: 0.6567\n",
            "Epoch 72/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 5.4532e-08 - accuracy: 1.0000 - val_loss: 9.2318 - val_accuracy: 0.6567\n",
            "Epoch 73/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 3.2274e-07 - accuracy: 1.0000 - val_loss: 9.6513 - val_accuracy: 0.6567\n",
            "Epoch 74/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 2.8205e-08 - accuracy: 1.0000 - val_loss: 9.6916 - val_accuracy: 0.6567\n",
            "Epoch 75/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.1272e-08 - accuracy: 1.0000 - val_loss: 9.9146 - val_accuracy: 0.6567\n",
            "Epoch 76/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 7.4261e-08 - accuracy: 1.0000 - val_loss: 9.7129 - val_accuracy: 0.6567\n",
            "Epoch 77/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 2.0133e-07 - accuracy: 1.0000 - val_loss: 10.1264 - val_accuracy: 0.6567\n",
            "Epoch 78/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.6262e-07 - accuracy: 1.0000 - val_loss: 9.9784 - val_accuracy: 0.6567\n",
            "Epoch 79/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 3.6238e-08 - accuracy: 1.0000 - val_loss: 9.9651 - val_accuracy: 0.6567\n",
            "Epoch 80/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.3208e-09 - accuracy: 1.0000 - val_loss: 9.9259 - val_accuracy: 0.6567\n",
            "Epoch 81/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0477 - accuracy: 0.9946 - val_loss: 10.8241 - val_accuracy: 0.6567\n",
            "Epoch 82/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.9252e-07 - accuracy: 1.0000 - val_loss: 11.1005 - val_accuracy: 0.6567\n",
            "Epoch 83/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 2.0461e-07 - accuracy: 1.0000 - val_loss: 10.7673 - val_accuracy: 0.6567\n",
            "Epoch 84/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.6047e-07 - accuracy: 1.0000 - val_loss: 10.9737 - val_accuracy: 0.6567\n",
            "Epoch 85/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 4.8453e-08 - accuracy: 1.0000 - val_loss: 11.1286 - val_accuracy: 0.6567\n",
            "Epoch 86/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 8.2622e-08 - accuracy: 1.0000 - val_loss: 11.1209 - val_accuracy: 0.6567\n",
            "Epoch 87/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.0707e-06 - accuracy: 1.0000 - val_loss: 11.1044 - val_accuracy: 0.6567\n",
            "Epoch 88/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.0360e-06 - accuracy: 1.0000 - val_loss: 12.6049 - val_accuracy: 0.6418\n",
            "Epoch 89/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3569e-07 - accuracy: 1.0000 - val_loss: 12.4565 - val_accuracy: 0.6418\n",
            "Epoch 90/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3677e-06 - accuracy: 1.0000 - val_loss: 10.2856 - val_accuracy: 0.6567\n",
            "Epoch 91/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 4.4171e-08 - accuracy: 1.0000 - val_loss: 10.3823 - val_accuracy: 0.6567\n",
            "Epoch 92/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 7.1314e-09 - accuracy: 1.0000 - val_loss: 10.3670 - val_accuracy: 0.6567\n",
            "Epoch 93/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.6927e-08 - accuracy: 1.0000 - val_loss: 10.4202 - val_accuracy: 0.6567\n",
            "Epoch 94/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.3338e-08 - accuracy: 1.0000 - val_loss: 10.3654 - val_accuracy: 0.6567\n",
            "Epoch 95/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 6.7863e-09 - accuracy: 1.0000 - val_loss: 10.3849 - val_accuracy: 0.6567\n",
            "Epoch 96/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 4.1161e-09 - accuracy: 1.0000 - val_loss: 10.3049 - val_accuracy: 0.6567\n",
            "Epoch 97/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 4.0145e-08 - accuracy: 1.0000 - val_loss: 10.5121 - val_accuracy: 0.6567\n",
            "Epoch 98/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.8220e-09 - accuracy: 1.0000 - val_loss: 10.5764 - val_accuracy: 0.6567\n",
            "Epoch 99/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3937e-07 - accuracy: 1.0000 - val_loss: 10.3522 - val_accuracy: 0.6567\n",
            "Epoch 100/100\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 6.9404e-08 - accuracy: 1.0000 - val_loss: 12.7745 - val_accuracy: 0.6418\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywBpZ_mXCRLi",
        "outputId": "d002cd05-e396-44c3-c8ff-658c2b81c728"
      },
      "source": [
        "score = model.evaluate(test_data, test_label)\n",
        "score_1 = model_1.evaluate(test_data, test_label)\n"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7666 - accuracy: 0.3536\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 7.3694 - accuracy: 0.8512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6ib7-TgDYKY"
      },
      "source": [
        "predictions=model_1.predict(test_data)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0ldSn5VD3us"
      },
      "source": [
        "y_pred = (predictions > 0.5)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErnsldNaD5pb",
        "outputId": "115ed577-77f4-4293-a705-6e4afe481a75"
      },
      "source": [
        "tf.math.confusion_matrix(\n",
        "    test_label, y_pred, num_classes=2, weights=None, dtype=tf.dtypes.int32,\n",
        "    name=None\n",
        ")"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
              "array([[31, 11],\n",
              "       [ 4, 63]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRwEF7NgELBp",
        "outputId": "a758e512-15bf-40ed-d067-80ac4b989eda"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(test_label,y_pred))"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.89      0.74      0.81        42\n",
            "         1.0       0.85      0.94      0.89        67\n",
            "\n",
            "    accuracy                           0.86       109\n",
            "   macro avg       0.87      0.84      0.85       109\n",
            "weighted avg       0.86      0.86      0.86       109\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}